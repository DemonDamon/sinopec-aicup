# 岩性识别类别不均衡处理策略调研日志


## 1. 采样方法 (Sampling Methods)

### SMOTE (Synthetic Minority Over-sampling Technique) 及其变种

**原理**: SMOTE算法通过分析少数类样本，并根据其近邻人工合成新的样本添加到数据集中，从而解决类别不平衡问题。它不是简单地复制现有样本，而是在少数类样本之间进行插值来生成新的合成样本。具体而言，对于每一个少数类样本 `x`，算法首先计算其 `k` 个近邻，然后从这些近邻中随机选择一个样本 `~x`，并通过线性插值 `x_new = x + lambda * (~x - x)` 来生成新的合成样本，其中 `lambda` 是 `(0, 1)` 之间的随机数 [[ref]](https://blog.csdn.net/wjjc1017/article/details/135361058) [[ref]](https://zhuanlan.zhihu.com/p/296632599) [[ref]](https://www.cnblogs.com/massquantity/p/9382710.html)。

**优点**: 
*   克服了简单复制少数类样本可能导致的过拟合问题 [[ref]](https://zhuanlan.zhihu.com/p/296632599)。
*   在一定程度上弥补了少数类样本的极端不足，提高了模型对少数类的识别能力 [[ref]](https://www.cnblogs.com/massquantity/p/9382710.html)。

**缺点/挑战**: 
*   **近邻选择的盲目性**: SMOTE算法在选择近邻时存在一定的盲目性，K值的确定需要反复测试 [[ref]](https://zhuanlan.zhihu.com/p/296632599)。
*   **分布边缘化问题**: 容易产生分布边缘化问题，即合成样本可能处于少数类样本分布的边缘，从而模糊了多数类和少数类之间的边界，增加了分类难度 [[ref]](https://zhuanlan.zhihu.com/p/296632599)。
*   **未考虑多数类样本分布**: 只是在同类近邻之间插值，未考虑少数类样本周围多数类样本的分布情况 [[ref]](https://zhuanlan.zhihu.com/p/296632599)。

**SMOTE 变种**: 为了解决SMOTE的缺陷，衍生出了多种改进算法，如 Borderline-SMOTE, SMOTE-Tomek, SMOTE-ENN 等。这些变种旨在生成更有效的样本，例如 Borderline-SMOTE 查找适当区域进行插值以解决样本重叠问题 [[ref]](https://zhuanlan.zhihu.com/p/296632599) [[ref]](https://www.cnblogs.com/massquantity/p/9382710.html)。

**与 Macro F1-Score 的关联**: SMOTE及其变种通过平衡类别分布，提高了模型对少数类的召回率，这通常有助于提升Macro F1-Score，因为Macro F1-Score是对每个类别的F1-Score进行算术平均，对所有类别同等看待。通过增加少数类的样本数量，模型能够更好地学习少数类的特征，从而改善少数类的分类性能。

### 欠采样 (Under-sampling) 及其变种 (Random Under-sampling, Tomek Links)

**原理**: 欠采样通过减少多数类样本的数量来实现样本均衡。直接的方法是随机删除一些多数类样本。此外，还存在原型选择方法如 `NearMiss`，它从多数类中选择最具代表性的样本进行训练，以缓解随机欠采样中的信息丢失问题 [[ref]](https://zhuanlan.zhihu.com/p/296632599)。

**Tomek Links**: Tomek Links是一种欠采样技术，用于识别并移除那些“噪声”或“重叠”的样本。如果一对异类样本（一个多数类，一个少数类）互为最近邻，则称它们构成一个Tomek Link。删除多数类中的Tomek Link样本可以清除边界噪音，使类别边界更清晰 [[ref]](https://blog.csdn.net/wjjc1017/article/details/135361058) [[ref]](https://zhuanlan.zhihu.com/p/159080497)。Tomek Links通常与其他采样方法（尤其是SMOTE）结合使用，以达到更好的平衡效果 [[ref]](https://blog.csdn.net/wjjc1017/article/details/135361058)。

**优点**: 
*   减少训练数据量，可能加速模型训练。
*   Tomek Links 可以帮助清理类别边界，改善模型泛化能力 [[ref]](https://blog.csdn.net/wjjc1017/article/details/135361058)。

**缺点**: 
*   随机欠采样可能导致多数类样本信息丢失 [[ref]](https://zhuanlan.zhihu.com/p/296632599)。
*   `NearMiss-1` 和 `NearMiss-2` 计算开销大，且 `NearMiss-1` 易受离群点影响 [[ref]](https://zhuanlan.zhihu.com/p/296632599)。

**与 Macro F1-Score 的关联**: 欠采样特别是结合了Tomek Links等清理机制的欠采样，可以优化类别边界，减少多数类对分类器的干扰，间接有助于改善少数类的分类性能，从而可能提升Macro F1-Score。

### 过采样与欠采样结合 (如 SMOTE-Tomek)

**原理**: 结合过采样和欠采样方法通常能取得更好的效果。例如，先使用SMOTE增加少数类样本，然后使用欠采样（如Tomek Links）减少多数类样本，并清理噪声，使类别边界更清晰 [[ref]](https://blog.csdn.net/wjjc1017/article/details/135361058)。

**优点**: 
*   综合两者的优势，既能增加少数类样本数量，又能清理数据、使类别界限更明确。
*   实验表明，SMOTE-Tomek等组合算法能同时提升模型的分类能力和各类别识别率 [[ref]](https://pmc.ncbi.nlm.nih.gov/articles/PMC9927194/) (snippet from google search, full text not scraped).

**与 Macro F1-Score 的关联**: 这种组合策略旨在更全面地解决类别不平衡问题，有望在提升少数类性能的同时，维持或改善多数类性能，从而对Macro F1-Score产生积极的综合影响。

## 2. 代价敏感学习 (Cost-Sensitive Learning)

**原理**: 代价敏感学习通过为不同类型的错误赋予“非均等代价”来解决类别不平衡问题，尤其是在不同错误具有不同后果的场景下。它使用一个代价矩阵 `cost_ij` 来表示将第 `i` 类样本预测为第 `j` 类样本的代价。通常，将少数类错分为多数类的代价会高于将多数类错分为少数类的代价，即 `cost_minority_majority > cost_majority_minority` [[ref]](https://zhuanlan.zhihu.com/p/296632599) [[ref]](https://zhuanlan.zhihu.com/p/37942047)。

**实现方式**: 
1.  **修改学习模型**: 直接修改具体的学习方法，如感知机、支持向量机、决策树和神经网络，使其能够适应不平衡数据下的学习。以代价敏感决策树为例，可从决策阈值、分裂标准和剪枝方面引入代价矩阵进行改进 [[ref]](https://zhuanlan.zhihu.com/p/296632599)。
2.  **后处理**: 将代价敏感学习视为分类结果的后处理，传统模型学习完成后，以最小化损失为目标对结果进行调整。这要求分类器能够输出概率 [[ref]](https://zhuanlan.zhihu.com/p/296632599)。
3.  **权重调整**: 从预处理角度，调整样本权重，使分类器满足代价敏感特性。例如，基于Adaboost的权重更新策略可以融入代价敏感性 [[ref]](https://zhuanlan.zhihu.com/p/296632599)。

**在XGBoost, LightGBM等模型中设置类别权重**: 
*   **`class_weight` 参数**: 许多机器学习模型（包括树形模型如XGBoost、LightGBM、随机森林）允许通过 `class_weight` 参数来设置不同类别的权重。对少数类别赋予更高的权重（即更高的错误分类成本），可以促使模型更关注少数类的分类 [[ref]](https://zhuanlan.zhihu.com/p/37942047) [[ref]](https://cloud.tencent.com/developer/article/2550542)。
*   **`sample_weight` 参数**: 类似的，也可以通过 `sample_weight` 参数为每个样本设置不同的权重，从而在训练过程中间接实现代价敏感 [[ref]](https://www.cnblogs.com/apachecn/p/19073428) (snippet, full content insufficient to extract details).
*   **实践案例**: 在随机森林中，通过 `class_weight` 参数可以有效地提升模型性能，防止其偏向多数类别 [[ref]](https://zhuanlan.zhihu.com/p/37942047)。对于XGBoost和LightGBM，同样可以通过设置 `class_weight` 或 `scale_pos_weight` (针对二分类少数类) 等参数来实现代价敏感学习。在腾讯云的文章中也提到了使用过采样、欠采样以及`class_weight`来解决类别不平衡问题 [[ref]](https://cloud.tencent.com/developer/article/2550542)。

**与 Macro F1-Score 的关联**: 代价敏感学习直接调整了模型对不同类别错误分类的重视程度。通过增加对少数类误分类的惩罚，模型会更倾向于正确分类少数类，从而提高少数类的召回率和精度。由于Macro F1-Score是对所有类别F1-Score的平均，这种策略能够直接提升Macro F1-Score，因为它确保了少数类别的性能不会被淹没在多数类别的优异性能中。

## 3. 算法与损失函数 (Algorithms and Loss Functions)

### Focal Loss

**原理**: Focal Loss 是一种专门设计用于处理类别极端不平衡问题的损失函数，它是在标准交叉熵损失函数的基础上改进而来的。Focal Loss通过降低易分类样本（多数类或容易正确分类的样本）的权重，使得模型在训练时更专注于那些难以分类的样本（通常是少数类样本）[[ref]](https://zhuanlan.zhihu.com/p/296632599) [[ref]](https://kexue.fm/archives/7708/comment-page-1)。

**公式**: 二分类的Focal Loss基本形式为：
$$FL(p_t) = -\alpha_t (1 - p_t)^{\gamma} \log(p_t)$$
其中：
*   `pt` 是模型预测的概率。
*   `(1 - pt)^γ` 是**调制因子 (modulating factor)**。当 `pt` 接近1（即样本易于分类）时，`(1 - pt)^γ` 会变得很小，从而降低这些易分类样本的损失贡献。当 `pt` 较小（即样本难于分类）时，`(1 - pt)^γ` 接近1，损失贡献基本不变，使得模型更关注这些样本。`γ` (gamma) 是一个可调参数，用于调节调制因子降低易分类样本权重的速率。通常 `γ` 设为2效果最佳 [[ref]](https://zhuanlan.zhihu.com/p/296632599)。
*   `αt` 是**平衡因子 (balancing factor)**，用于平衡正负样本本身的比例不均，解决正负样本数量差异的问题。它对少数类给予更高的权重。通常 `α` 设为0.25 [[ref]](https://zhuanlan.zhihu.com/p/296632599) (Note: `alpha` is a constant or dynamically assigned per class, e.g., `alpha_t = alpha` if `t` is positive class, `1-alpha` if negative class).

**优点**: 
*   有效解决极端类别不平衡问题，特别是在正负样本数量差距悬殊的场景下（例如目标检测中的背景与前景）[[ref]](https://kexue.fm/archives/7708/comment-page-1)。
*   强制模型更加关注“硬样本”（hard examples），这些样本对于学习更精细的决策边界至关重要。

**缺点/挑战**: 
*   引入了两个超参数 `α` 和 `γ`，它们相互影响，可能增加调参成本 [[ref]](http://www.360doc.com/content/22/1209/00/7673502_1059528034.shtml) (snippet from google search, full text not scraped).
*   Focal Loss 是一种静态损失函数，超参数可能无法适用于所有数据集 [[ref]](http://www.360doc.com/content/22/1209/00/7673502_1059528034.shtml) (snippet).

**与 Macro F1-Score 的关联**: Focal Loss 通过增强模型对少数类和难分类样本的学习能力，直接提高了这些类别的分类性能。由于Macro F1-Score是对每个类别的F1-Score进行平均，Focal Loss能够显著提升Macro F1-Score，因为它确保了模型不会因为大多数易分类样本而忽略了少数类别的学习，从而改善了所有类别的均衡表现。

### F1-Score 作为损失函数或优化目标

在语义分割等任务中，可以将F1-Score（或其变体如 `1-F1Score`）直接用作损失函数进行优化 [[ref]](https://blog.csdn.net/m0_37910705/article/details/102598832) (snippet from google search, full text not scraped)。虽然F1-Score本身通常用作评价指标，但将其融入损失函数可以使模型训练过程直接与F1-Score最大化目标对齐。这对于Macro F1-Score尤其有利，因为它促使模型在训练时就关注到所有类别的精确率和召回率的平衡。当然，将其用作损失函数时需要注意其可导性问题。

### 其他专门设计算法

*   **集成学习与类别不平衡**: `BalancedBaggingClassifier` 允许在训练集合的每个分类器之前对数据集的每个子集进行重新采样，从而在集成学习的框架下解决数据不平衡问题 [[ref]](https://zhuanlan.zhihu.com/p/296632599)。
*   **对现有算法的改进**: 许多针对不平衡数据的机器学习方法致力于对现有的分类器进行改造，使其能够适应不平衡数据的学习 [[ref]](http://tis.hrbeu.edu.cn/Upload/PaperUpLoad/fb14d34d-2c87-49cf-a5a0-22cfda59d0cd.pdf) (snippet). 这与代价敏感学习对模型改造的思路是类似的。

**总结不同策略的侧重点和潜在影响**: 

*   **采样方法**: 主要侧重于**数据层面**的调整，通过改变数据集的类别分布来“欺骗”标准分类器，使其能够更好地学习少数类特征。它们普遍有助于提高少数类的召回率，从而提升Macro F1-Score。过采样可能增加过拟合风险，欠采样可能导致信息丢失。组合采样（如SMOTE-Tomek）通常效果最佳，因为它们兼顾了样本数量平衡和类别边界清晰。

*   **代价敏感学习**: 侧重于**模型层面**的调整，通过引入不均衡的错误代价来改变模型的学习偏好。它直接指导模型在训练时对少数类错误给予更高的惩罚，从而促使模型更积极地识别少数类。这通常能在不改变原始数据分布的情况下，提升少数类的分类性能，直接作用于Macro F1-Score。通过在XGBoost和LightGBM中设置`class_weight`或`scale_pos_weight`是常用的有效策略。

*   **算法与损失函数 (如 Focal Loss)**: 侧重于**算法核心机制**的改造，特别是损失函数的设计，以适应类别不均衡的挑战。Focal Loss通过动态调整样本权重，使模型在学习过程中自动聚焦于难分类的样本，从而提高少数类的分类准确性。它在极端类别不平衡问题上表现优异，并能直接优化所有类别的F1-Score，进而提高Macro F1-Score。将F1-Score直接融入损失函数是更直接的优化目标，但需要解决可导性问题。

**对 Macro F1-Score 的影响分析**: 
所有这些策略的核心目标都是改善模型在少数类别上的性能，因为Macro F1-Score对所有类别（包括少数类）一视同仁，其表现强烈依赖于少数类的F1-Score。 

*   **采样方法**通过提供更多或更清晰的少数类信息来提高其可学习性。
*   **代价敏感学习**通过惩罚少数类错误来激励模型更好地识别少数类。
*   **Focal Loss**通过重加权损失来迫使模型关注难分类（通常是少数类）样本。

在岩性识别任务中，Macro F1-Score作为评价指标，意味着我们需要确保模型对所有岩性类别的识别能力都相对均衡。因此，单一地提高多数类的准确率是不足的。上述策略可以单独使用，但往往组合使用能达到最佳效果。例如，先用SMOTE处理样本不平衡，然后使用带有`class_weight`的XGBoost/LightGBM进行训练，或者在深度学习模型中采用Focal Loss，都可能在实际应用中取得良好的Macro F1-Score表现。选择哪种策略或策略组合，应结合数据具体情况、模型类型和计算资源进行权衡。