"""
å¿«é€Ÿé«˜æ•ˆçš„å¤šç®—æ³•é›†æˆæ¨¡å‹
ä½¿ç”¨LightGBMã€CatBoostã€XGBoostä¸‰ç§ç®—æ³•è¿›è¡Œé›†æˆ
åŸºäºç©ºé—´è¿ç»­æ€§ç‰¹å¾ï¼Œä¼˜åŒ–è®­ç»ƒæ—¶é—´ï¼Œä¿æŒé«˜æ€§èƒ½
"""

import numpy as np
import pandas as pd
import os
from datetime import datetime
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

from config import RANDOM_STATE, OUTPUT_PATHS, MODEL_PATHS
from utils import load_data, set_random_seed, save_model
from base_models import BaseModelTrainer

set_random_seed(RANDOM_STATE)

def create_spatial_features(data):
    """åˆ›å»ºç²¾ç®€ä½†é«˜æ•ˆçš„ç©ºé—´è¿ç»­æ€§ç‰¹å¾"""
    print("ğŸ”¬ åˆ›å»ºç©ºé—´è¿ç»­æ€§ç‰¹å¾...")
    
    # æŒ‰äº•å’Œæ·±åº¦æ’åº
    data_sorted = data.sort_values(['WELL', 'DEPTH']).copy()
    
    # åŸºç¡€ç‰¹å¾
    features = data_sorted[['DEPTH', 'SP', 'GR', 'AC']].copy()
    
    # ä¸ºæ¯å£äº•åˆ›å»ºç©ºé—´ç‰¹å¾
    for well in data_sorted['WELL'].unique():
        well_mask = data_sorted['WELL'] == well
        well_data = data_sorted[well_mask].copy()
        
        if len(well_data) < 3:
            continue
            
        # ç²¾é€‰çš„æ»‘åŠ¨çª—å£ç‰¹å¾ (åªç”¨æœ€æœ‰æ•ˆçš„)
        for window in [5, 11]:  # å‡å°‘çª—å£æ•°é‡
            for col in ['SP', 'GR', 'AC']:
                # æ»‘åŠ¨å¹³å‡
                features.loc[well_mask, f'{col}_ma_{window}'] = (
                    well_data[col].rolling(window, center=True, min_periods=1).mean()
                )
                # æ»‘åŠ¨æ ‡å‡†å·®
                features.loc[well_mask, f'{col}_std_{window}'] = (
                    well_data[col].rolling(window, center=True, min_periods=1).std().fillna(0)
                )
        
        # æ¢¯åº¦ç‰¹å¾
        for col in ['SP', 'GR', 'AC']:
            gradient = np.gradient(well_data[col].values)
            features.loc[well_mask, f'{col}_gradient'] = gradient
    
    # æ·±åº¦æ ‡å‡†åŒ–
    for well in data_sorted['WELL'].unique():
        well_mask = data_sorted['WELL'] == well
        well_depths = features.loc[well_mask, 'DEPTH']
        if len(well_depths) > 1:
            features.loc[well_mask, 'DEPTH_normalized'] = (
                (well_depths - well_depths.min()) / (well_depths.max() - well_depths.min())
            )
        else:
            features.loc[well_mask, 'DEPTH_normalized'] = 0.5
    
    # å²©æ€§è¯†åˆ«ç‰¹å¾ç»„åˆ
    features['GR_SP_ratio'] = features['GR'] / (features['SP'] + 1e-6)
    features['sandstone_indicator'] = (
        (features['GR'] < features['GR'].quantile(0.3)) & 
        (features['SP'] > features['SP'].quantile(0.7))
    ).astype(int)
    
    # æ¢å¤åŸå§‹é¡ºåº
    features = features.reindex(data.index)
    
    print(f"âœ… åˆ›å»ºäº† {features.shape[1]} ä¸ªç‰¹å¾")
    return features

def train_fast_ensemble():
    """å¿«é€Ÿè®­ç»ƒé›†æˆæ¨¡å‹"""
    print("ğŸ¯ è®­ç»ƒå¿«é€Ÿç©ºé—´è¿ç»­æ€§é›†æˆæ¨¡å‹...")
    
    # 1. åŠ è½½æ•°æ®
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    train_data = load_data('train')
    test_data = load_data('test')
    
    # 2. åˆ›å»ºç©ºé—´ç‰¹å¾
    train_features = create_spatial_features(train_data)
    test_features = create_spatial_features(test_data)
    
    # 3. å‡†å¤‡è®­ç»ƒæ•°æ®
    from config import TARGET_COLUMN, ID_COLUMN
    
    X = train_features
    y = train_data[TARGET_COLUMN]
    
    # ç¡®ä¿æµ‹è¯•é›†æœ‰ç›¸åŒçš„ç‰¹å¾
    common_features = list(set(X.columns) & set(test_features.columns))
    X = X[common_features]
    X_test = test_features[common_features]
    
    print(f"ğŸ“ˆ æœ€ç»ˆç‰¹å¾æ•°é‡: {len(common_features)}")
    
    # 4. ç‰¹å¾æ ‡å‡†åŒ–
    print("ğŸ”§ ç‰¹å¾æ ‡å‡†åŒ–...")
    scaler = StandardScaler()
    X_scaled = pd.DataFrame(
        scaler.fit_transform(X), 
        columns=X.columns, 
        index=X.index
    )
    X_test_scaled = pd.DataFrame(
        scaler.transform(X_test), 
        columns=X_test.columns, 
        index=X_test.index
    )
    
    # 5. å®šä¹‰3ç§ä¸åŒç®—æ³•çš„æ¨¡å‹é…ç½®
    model_configs = [
        {
            'name': 'lightgbm_fast',
            'model_type': 'lightgbm',
            'params': {
                'objective': 'multiclass',
                'num_class': 3,
                'boosting_type': 'gbdt',
                'num_leaves': 31,
                'learning_rate': 0.1,
                'feature_fraction': 0.9,
                'bagging_fraction': 0.8,
                'bagging_freq': 5,
                'min_child_samples': 20,
                'n_estimators': 800,
                'random_state': 42,
                'verbose': -1,
                'force_col_wise': True,
                'early_stopping_rounds': None  # æ˜ç¡®ç¦ç”¨early stopping
            }
        },
        {
            'name': 'catboost_fast',
            'model_type': 'catboost',
            'params': {
                'iterations': 800,
                'depth': 8,
                'learning_rate': 0.08,
                'l2_leaf_reg': 5,
                'bootstrap_type': 'Bernoulli',
                'subsample': 0.8,
                'colsample_bylevel': 0.9,
                'random_seed': 123,
                'verbose': False
            }
        },
        {
            'name': 'xgboost_fast',
            'model_type': 'xgboost',
            'params': {
                'objective': 'multi:softprob',
                'num_class': 3,
                'max_depth': 8,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.9,
                'min_child_weight': 1,
                'n_estimators': 800,
                'random_state': 456,
                'verbosity': 0,
                'early_stopping_rounds': None  # æ˜ç¡®ç¦ç”¨early stopping
            }
        }
    ]
    
    # 6. å¿«é€Ÿäº¤å‰éªŒè¯è¯„ä¼° (åªç”¨3æŠ˜)
    print("\\nğŸ”„ å¿«é€Ÿè¯„ä¼°å„ä¸ªåŸºç¡€æ¨¡å‹...")
    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)  # å‡å°‘æŠ˜æ•°
    
    model_scores = {}
    trained_models = {}
    
    for config in model_configs:
        print(f"\\nğŸ“Š è¯„ä¼° {config['name']}...")
        cv_scores = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y)):
            X_train_fold = X_scaled.iloc[train_idx]
            X_val_fold = X_scaled.iloc[val_idx]
            y_train_fold = y.iloc[train_idx]
            y_val_fold = y.iloc[val_idx]
            
            # è®­ç»ƒæ¨¡å‹
            trainer = BaseModelTrainer(model_type=config['model_type'], model_params=config['params'])
            trainer.train(X_train_fold, y_train_fold, early_stopping_rounds=None, verbose=False)
            
            # é¢„æµ‹å’Œè¯„ä¼°
            y_pred = trainer.predict(X_val_fold)
            f1 = f1_score(y_val_fold, y_pred, average='macro')
            cv_scores.append(f1)
        
        mean_f1 = np.mean(cv_scores)
        std_f1 = np.std(cv_scores)
        model_scores[config['name']] = {'mean': mean_f1, 'std': std_f1}
        
        print(f"  {config['name']}: {mean_f1:.4f} Â± {std_f1:.4f}")
        
        # è®­ç»ƒå®Œæ•´æ¨¡å‹
        trainer = BaseModelTrainer(model_type=config['model_type'], model_params=config['params'])
        trainer.train(X_scaled, y, early_stopping_rounds=None, verbose=False)
        trained_models[config['name']] = trainer
    
    # 7. é›†æˆé¢„æµ‹
    print("\\nğŸ”„ é›†æˆé¢„æµ‹...")
    
    # è½¯æŠ•ç¥¨é›†æˆ
    ensemble_predictions = []
    individual_predictions = {}
    
    for name, model in trained_models.items():
        pred_proba = model.model.predict_proba(X_test_scaled)
        individual_predictions[name] = pred_proba
        ensemble_predictions.append(pred_proba)
    
    # ç®€å•å¹³å‡
    avg_proba = np.mean(ensemble_predictions, axis=0)
    final_predictions = np.argmax(avg_proba, axis=1)
    
    # 8. åŠ æƒé›†æˆ
    print("\\nğŸ”„ åŠ æƒé›†æˆ...")
    weights = []
    for config in model_configs:
        weight = model_scores[config['name']]['mean']
        weights.append(weight)
    
    # å½’ä¸€åŒ–æƒé‡
    weights = np.array(weights)
    weights = weights / weights.sum()
    
    print("æ¨¡å‹æƒé‡:")
    for i, config in enumerate(model_configs):
        print(f"  {config['name']}: {weights[i]:.3f}")
    
    # åŠ æƒå¹³å‡
    weighted_proba = np.zeros_like(avg_proba)
    for i, (name, pred_proba) in enumerate(individual_predictions.items()):
        weighted_proba += weights[i] * pred_proba
    
    weighted_predictions = np.argmax(weighted_proba, axis=1)
    
    # 9. å¿«é€Ÿè¯„ä¼°é›†æˆæ•ˆæœ (åªç”¨ä¸€æ¬¡éªŒè¯)
    print("\\nğŸ”„ è¯„ä¼°é›†æˆæ•ˆæœ...")
    
    # ä½¿ç”¨ä¸€æ¬¡éšæœºåˆ†å‰²å¿«é€Ÿè¯„ä¼°
    from sklearn.model_selection import train_test_split
    X_train_quick, X_val_quick, y_train_quick, y_val_quick = train_test_split(
        X_scaled, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
    )
    
    # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    quick_predictions = []
    quick_weights = []
    
    for config in model_configs:
        trainer = BaseModelTrainer(model_type=config['model_type'], model_params=config['params'])
        trainer.train(X_train_quick, y_train_quick, early_stopping_rounds=None, verbose=False)
        
        pred_proba = trainer.model.predict_proba(X_val_quick)
        quick_predictions.append(pred_proba)
        quick_weights.append(model_scores[config['name']]['mean'])
    
    # åŠ æƒé›†æˆ
    quick_weights = np.array(quick_weights)
    quick_weights = quick_weights / quick_weights.sum()
    
    ensemble_proba = np.zeros_like(quick_predictions[0])
    for i, pred_proba in enumerate(quick_predictions):
        ensemble_proba += quick_weights[i] * pred_proba
    
    ensemble_pred = np.argmax(ensemble_proba, axis=1)
    ensemble_f1 = f1_score(y_val_quick, ensemble_pred, average='macro')
    
    print(f"\\nğŸ“Š é›†æˆæ¨¡å‹å¿«é€Ÿè¯„ä¼°F1: {ensemble_f1:.4f}")
    
    # 10. åˆ†æé¢„æµ‹åˆ†å¸ƒ
    print("\\nğŸ“Š é¢„æµ‹åˆ†å¸ƒ:")
    weighted_dist = pd.Series(weighted_predictions).value_counts().sort_index()
    for k, v in weighted_dist.items():
        print(f"  ç±»åˆ«{k}: {v} ({v/len(weighted_predictions)*100:.1f}%)")
    
    # 11. ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    submission_file = f"submission_fast_ensemble_{timestamp}.csv"
    submission_path = os.path.join(OUTPUT_PATHS['predictions'], submission_file)
    
    if ID_COLUMN in test_data.columns:
        test_ids = test_data[ID_COLUMN].values
    else:
        test_ids = list(range(len(X_test_scaled)))
    
    submission_df = pd.DataFrame({
        'id': test_ids,
        'predict': weighted_predictions
    })
    submission_df.to_csv(submission_path, index=False)
    
    print(f"\\nğŸ’¾ æäº¤æ–‡ä»¶: {submission_path}")
    
    return {
        'ensemble_f1': ensemble_f1,
        'model_scores': model_scores,
        'weights': weights,
        'submission_path': submission_path,
        'distribution': weighted_dist.to_dict()
    }

if __name__ == "__main__":
    result = train_fast_ensemble()
    
    print(f"\\nğŸ‰ å¤šç®—æ³•é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š é›†æˆæ¨¡å‹F1: {result['ensemble_f1']:.4f}")
    print(f"ğŸ“„ æ¨èæäº¤: {result['submission_path']}")
    print(f"âš¡ ä½¿ç”¨LightGBM+CatBoost+XGBoostä¸‰ç§ç®—æ³•é›†æˆ!")